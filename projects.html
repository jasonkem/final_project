<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>My Python Projects</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <a href="#maincontent" class="skip-link">Skip to Main Content</a>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="projects.html">Projects</a></li>
                <li><a href="essays.html">Essays</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </nav>
        <h1 class="centered-heading">SI Projects</h1>
    </header>
    <main id="maincontent">
        <div class="folders-container">
            <a href="#si370">
                <img src="images/SI_370-removebg-preview.png" alt="SI 370" class="folder-image">
            </a>
            <a href="#si330">
                <img src="images/SI_330-removebg-preview.png" alt="SI 330" class="folder-image">
            </a>
            </a>
            <a href="#si206">
                <img src="images/SI_206-removebg-preview.png" alt="SI 206" class="folder-image">
            </a>
            <a href="#si106">
                <img src="images/SI_106-removebg-preview.png" alt="SI 206" class="folder-image">
            </a>
        </div>
        <section id="si370">
            <h2 class="centered-subheading">SI 370 Projects</h2>
            <div class="homework-container hw-right">
                <div class="hw-desc">
                    <h3>Homework 1</h3>
                    <p>SI 370 - Homework #1: Data Manipulation and Preliminary Analyses delves into the world of data science job salaries, providing students with practical experience in data exploration and analysis. This assignment is meticulously designed to equip students with the skills necessary to navigate and interrogate complex datasets, utilizing both Python and the powerful pandas library. The emphasis is on real-world application, encouraging students to seek answers within the data through manipulation and analysis techniques taught in class and explored further through self-directed learning.</p>

                    <p>The assignment begins with acquiring the dataset from Kaggle, a renowned platform for data scientists. The dataset, named ds_salaries.csv, contains comprehensive details about job salaries in the data science domain, presenting an intriguing challenge for analysis. The task set forth involves a series of questions, each aimed at unraveling different facets of the dataset. The grading rubric is clear and stringent, rewarding not only the correctness and completeness of answers but also the elegance and readability of the code, adherence to PEP 8 guidelines, and the clarity and precision of written interpretations.</p>
                    <p>A notable aspect of this homework is its incremental difficulty and distribution of points, with an initial focus on loading and describing the dataset effectively. This foundational task sets the stage for deeper analysis, including identifying the median salaries across different job titles and comparing the representation of countries within the dataset. The use of external libraries like pycountry for enriching the dataset with complete country names adds an extra layer of complexity and realism to the task, bridging the gap between classroom learning and real-world data science work.</p>
                    <p>Furthermore, the assignment challenges students to think critically about the data, questioning the relationship between employee residence and company location, and exploring the distribution of salaries through quartile analysis. This not only tests their technical prowess with pandas but also their analytical thinking and ability to draw meaningful conclusions from the data.</p>
                    <p>In summary, SI 370 - Homework #1 is a comprehensive exploration of data science job salaries, demanding a blend of technical skills, analytical thinking, and clear communication. It reflects the real challenges faced by data scientists and prepares students for the intricacies of data manipulation and analysis in their future careers.</p>
                </div>
                <iframe src="Projects/SI 370/SI_370_Homework_1.html" class="hw-img"></iframe>
            </div>
            <div class="homework-container hw-left">
                <iframe src="Projects/SI 370/SI_370_Homework_2.html" class="hw-img"></iframe>
                <div class="hw-desc">
                    <h3>Homework 2</h3>
                    <p>In SI 370 - Homework #2: Data Visualization, I embarked on a journey through the world of data visualization with a focus on the MovieLens dataset. This assignment was an intriguing blend of data exploration, manipulation, and visualization, presenting an opportunity to dive deep into the nuances of movie ratings, genres, and production trends over decades. My task was to not only create meaningful visualizations using pandas, Seaborn, and matplotlib but also to think critically about the data at hand and how best to represent it visually.</p>
                    <p>The challenge began with the extraction of the dataset, which set the stage for a series of questions each aimed at uncovering different aspects of the movie industry as captured in the data. From discerning the popularity of genres to analyzing the distribution of movie productions across decades, each question pushed me to apply the data manipulation and analysis skills I've learned in class, supplemented by my own research and exploration of documentation and online resources.</p>
                    <p>A significant part of this homework was the planning process. It wasn't just about applying code to data; it was about understanding the questions at a deeper level and devising a strategy to tackle them effectively. This approach helped me to not only answer the questions but also to engage with the data in a more meaningful way, uncovering insights that were not immediately obvious.</p>
                    <p>The assignment's grading rubric, with its emphasis on correctness, code quality, and clear written interpretations, served as a guide for me to strive for excellence in both my coding and explanatory efforts. I found myself going back and forth between my code and the narrative explanations, ensuring that my visualizations were not just accurate but also tell a compelling story about the data.</p>
                    <p>One of the most rewarding aspects of this homework was the opportunity to explain my findings in plain English. This was more than an academic exercise; it was a chance to practice communicating complex data insights in a way that's accessible to an educated professional who may not have a background in data science. It underscored the importance of data visualization not just as a technical skill but as a bridge between data and decision-making. In summary, Homework #2 was a comprehensive foray into data visualization that challenged me to blend technical skills with critical thinking and effective communication. It was an enriching experience that deepened my appreciation for the power of visual storytelling in data science.</p>
                </div>
            </div>
            <div class="homework-container hw-right">
                <div class="hw-desc">
                    <h3>Homework 3</h3>
                    <p>In the whirlwind that is the intersection of music and data analytics, SI 370 Homework #3 delves into the captivating world of Taylor Swift's discography as cataloged on Spotify. My exploration through this dataset wasn't just an academic exercise but a genuine journey into the intricate details that Spotify's API holds on Taylor Swift's albums. Initially, the dataset presents a treasure trove of attributes from the basic—such as song names, album titles, and release dates—to the more complex features like acousticness, danceability, energy, and so forth, each painting a unique picture of her songs' characteristics.</p>
                    <p>My task was to dissect this dataset, seeking out patterns, anomalies, and correlations within her music over the years. Armed with pandas for data manipulation, and seaborn alongside matplotlib for visualization, I embarked on a series of analyses that ranged from the distribution of song popularity to the musicality and instrumental texture of her tracks. A particular point of intrigue was the differentiation between original songs and their re-recorded versions; this comparison not only highlighted Swift's evolution as an artist but also reflected the changing tastes of her audience.</p>
                    <p>One of the most enlightening aspects of this homework was uncovering the relationship between various musical features and song popularity. For instance, analyzing whether a song's placement within an album influenced its popularity or how the danceability of a track could potentially affect its reception. Each query pushed me to think critically, not just about the data and its implications but also about the broader cultural and economic factors at play in the music industry.</p>
                    <p>Perhaps the most challenging yet rewarding part was theorizing about Taylor Swift's impact beyond music—specifically, her influence on football's popularity. This bonus question offered a fascinating avenue for speculative analysis, prompting me to consider the data and methodologies required to explore such a cross-domain impact thoroughly. My approach would involve examining NFL viewership patterns, merchandise sales, and social media sentiment analysis before and after significant events featuring Swift to gauge any shifts in football's popularity, particularly among demographics that traditionally may not follow the sport closely.</p>
                    <p>In reflecting on this assignment, it's evident that the blend of technical data analysis with the creative and dynamic world of music and pop culture not only enhances my analytical skills but also enriches my understanding of the interplay between art, technology, and society. Through the lens of Taylor Swift's Spotify dataset, I've navigated the complexities of data analytics, emerging with a deeper appreciation for both the science behind the numbers and the stories they tell.</p>
                </div>
                <iframe src="Projects/SI 370/SI_370_Homework_3.html" class="hw-img"></iframe>
            </div>
            <div class="homework-container hw-left">
                <iframe src="Projects/SI 370/SI_370_Homework_4.html" class="hw-img"></iframe>
                <div class="hw-desc">
                    <h3>Homework 4</h3>
                    <p>Embarking on SI 370 Homework #4 felt like setting sail into the vast ocean of machine learning, with its complex currents and unpredictable weather patterns. Tasked with applying a plethora of techniques on graduate admissions data, the journey was both challenging and enlightening. I began with exploratory data analyses, where the graphical and numerical overviews laid the foundation of my understanding. It was like charting the map before the voyage, ensuring no stone was left unturned.</p>
                    <p>The heart of this assignment was modeling the chance of admission through linear regression. The preparation required for this task—scaling the data and conducting an 80-20 train-test split—was meticulous. The outcome, a model explaining over 80% of the variance in admission chances, was a beacon of success in the murky waters of predictive analytics. Yet, the real test of my analytical prowess came with the interpretation of this model, navigating through coefficients to understand the impact of each variable.</p>
                    <p>The creation of an "admitted" variable, based on a threshold I deemed reasonable, was akin to deciding on the right moment to tack in sailing: a strategic decision that affects the course ahead. This decision underscored the importance of subjective judgment in data science, reminding me that behind every dataset and model lies a series of human decisions.</p>
                    <p>Agglomerative clustering and k-means clustering further tested my skills, as I sought to group applicants in meaningful ways based on their data. The silhouette scores served as my North Star, guiding me to the optimal number of clusters. This part of the journey was particularly rewarding, offering insights into the structure of the data that were not immediately apparent from the outset.</p>
                    <p>Perhaps the most visually captivating part of this assignment was the t-SNE analysis. Like spotting land after days at sea, the t-SNE plot provided a clear visual representation of the data's complexity. The differentiation between admitted and not admitted students emerged organically through the analysis, painting a vivid picture of the admissions landscape.</p>
                    <p>In reflecting on this assignment, I realized that machine learning is as much about the journey as it is about the destination. Each technique, from linear regression to clustering and t-SNE, offers a different lens through which to view the data. Just as a sailor must understand the sea to navigate it successfully, a data scientist must understand their tools to uncover the stories hidden within the data. Through this assignment, I've not only honed my technical skills but also deepened my appreciation for the art and science of machine learning.</p>
                </div>
            </div>
            <div class="homework-container hw-right">
                <div class="hw-desc">
                    <h3>Homework 5</h3>
                    <p>Embarking on SI 370 Homework #5, I dove into the intriguing world of machine learning classifiers with a blend of anticipation and determination. The challenge was set against the backdrop of a Kaggle competition, predicting which passengers aboard the Spaceship Titanic would be transported to an alternate dimension. This premise alone was enough to spark my curiosity and drive my ambition to excel.</p>
                    <p>Accepting the competition rules and downloading the data marked the beginning of my journey. With the datasets in hand, my initial task was to perform exploratory data analysis (EDA). Delving into the data, I visualized the distribution of various features, such as age, room service charges, and more, using histograms and a correlation heatmap. This phase was crucial for gaining insights into the relationships between different variables and the target outcome, 'Transported'.</p>
                    <p>The core of my assignment revolved around training, tuning, and ensembling machine learning models. After pre-processing the data and handling missing values through imputation, I embarked on model training. The Gradient Boosting Classifier, with its parameters finely tuned through GridSearchCV, emerged as a formidable contender. However, I didn't stop there. Understanding the power of ensemble methods, I also integrated a Multi-layer Perceptron Classifier and a Decision Tree Classifier into a Voting Classifier, seeking to leverage the strengths of each model.</p>
                    <p>The validation accuracy of my Voting Classifier was a testament to the effectiveness of ensemble methods. Yet, the real test came with the submission of my predictions to Kaggle, where the accuracy score awaited. This step was not just about seeing my name on the leaderboard but also about validating my skills and learning from the entire experience.</p>
                    <p>Reflecting on this assignment, I'm reminded of the dynamic and ever-evolving nature of machine learning. Each model offered its own perspective, and the ensemble approach underscored the adage that the whole is greater than the sum of its parts. Beyond the technical skills honed, this assignment taught me the importance of creativity and strategic thinking in data science.</p>
                </div>
                <iframe src="Projects/SI 370/SI_370_Homework_5.html" class="hw-img"></iframe>
            </div>
        </section>
        <section id="si330">
            <h2 class="centered-subheading">SI 330 Final Project</h2>
            <div class="homework-container hw-left">
                <iframe src="Projects/SI 301/Final Project File.html" class="hw-img"></iframe>
                <div class="hw-desc">
                    <h3>SI 330 Final Project Description</h3>
                    <p>Embarking on this assignment felt like diving into the deep end of data exploration and visualization, particularly focusing on the intriguing world of alternative fuel stations and electric vehicle (EV) population data. The task began with a simple yet crucial step: identifying relevant datasets within a designated directory. This preliminary step set the stage for a more detailed and complex analysis that would unfold.</p>
                    <p>Upon loading the alternative fuel stations dataset, I was immediately struck by the richness of the data. With columns ranging from the station's name and address to more nuanced details like the types of electric vehicle supply equipment (EVSE) numbers and EV network information, the dataset was a treasure trove of insights waiting to be unearthed. However, it also posed challenges due to the presence of mixed data types and the sheer volume of information. Each record told a story of an infrastructure piece critical to supporting the growing EV market, yet deciphering these tales required careful navigation through the dataset's complexities.</p>
                    <p>The journey continued with the examination of the Electric Vehicle Population Data, which provided a comprehensive overview of EVs registered in various locations. The dataset offered a snapshot of the EV landscape, including makes, models, types, and even the eligibility for clean alternative fuel vehicle programs. What struck me most was the blend of technical specifications with geographic and demographic information, painting a holistic picture of the EV ecosystem.</p>
                    <p>The exploration was not without its challenges, particularly when merging datasets based on geographic locations to identify potential correlations between EV counts and charging station availability. The task of merging and filtering data, while conceptually straightforward, demanded precision and thoughtful consideration of the underlying assumptions. It underscored the importance of clean, well-organized data for meaningful analysis.</p>
                    <p>One of the most enlightening moments came from visualizing the EV locations using geospatial data. This exercise brought the data to life, transforming abstract numbers and addresses into a vivid map that highlighted the distribution of EVs. It was a powerful reminder of the tangible impact of data analysis, offering visual insights that could inform policy, infrastructure planning, and even consumer behavior.</p>
                    <p>Throughout this assignment, I navigated through a sea of data, tools, and techniques, from handling mixed data types and large datasets to leveraging geospatial visualization. Each step, whether it was parsing through the alternative fuel stations data or mapping the electric vehicle population, offered unique insights and challenges. The experience reinforced the importance of adaptability, attention to detail, and the ability to derive meaningful narratives from raw data. As I concluded this exploratory journey, I was reminded of the profound potential of data analysis to not only inform but also to inspire action towards a more sustainable and connected world.</p>
                </div>
            </div>
        </section>
        <section id="si206">
            <h2 class="centered-subheading">SI 206 Project</h2>
            <div class="homework-container hw-right">
                <div class="hw-desc">
                    <h3>SI 206 Final Project</h3>
                    <p>In my UMSI 206 Final Project, I embarked on creating a sophisticated data integration system involving the collection, processing, and visualization of Michigan football game scores, weather conditions, and dates over a decade from 2009 to 2019. The project was built upon various Python libraries, including requests for API calls, sqlite3 for database operations, and matplotlib and plotly for data visualization, providing a comprehensive toolkit to handle the complex data involved.</p>
                    <p>My first step in this project was to establish a reliable method to retrieve data from the College Football Data API and the Weatherstack API. This involved crafting functions like get_michigan_date, get_michigan_score, and get_michigan_temp to fetch specific data points based on year and week parameters. The functions were designed to handle JSON responses effectively, extracting necessary information such as game dates, total scores, and average temperatures on game days.</p>
                    <p>To manage and store this data efficiently, I set up an SQLite database with three main tables: Dates, Temperature, and Scores. I implemented functions to create these tables and populate them with data batches, ensuring data integrity and facilitating easy access and manipulation. This structured storage allowed for seamless integration and querying of related data points across different tables.</p>
                    <p>The analytical aspect of the project involved joining these tables to correlate and analyze the collected data. I explored relationships between game scores and weather conditions using SQL queries to join data and statistical functions to calculate means and other relevant metrics. This analysis aimed to uncover trends or patterns that could be insightful for understanding the impact of weather on game performance.</p>
                    <p>Finally, I utilized powerful visualization tools to present my findings. Using matplotlib, I created histograms to display the distribution of scores and temperatures, which helped in visualizing the data's spread and central tendencies. Additionally, plotly was used to create interactive scatter plots and bar graphs, illustrating the relationship between scores and temperatures over the years in a more dynamic and engaging way.</p>
                    <p>This project not only enhanced my technical skills across several dimensions—data fetching, backend storage, data analysis, and visual reporting—but also provided valuable insights into the interplay between environmental conditions and sports performance. By handling every aspect of the project from data collection to visualization, I gained a comprehensive understanding of the data science workflow, which will be immensely beneficial for my future endeavors in the field.</p>
                </div>
                <iframe src="Projects/SI 206 Final.html" class="hw-img"></iframe>
            </div>
        </section>
        <section id="si106">
            <h2 class="centered-subheading">SI 106 Project</h2>
            <div class="homework-container hw-left">
                <iframe src="Projects/SI106_Final_Project_Notebook.html" class="hw-img"></iframe>
                <div class="hw-desc">
                    <h3>SI 106 Final Project</h3>
                    <p>In my final project for UMSI 106, I embarked on creating a vacation recommender system leveraging the OpenWeather API. This task involved asking users to input cities of interest and their ideal weather temperatures, with the program then determining which city's current weather conditions most closely matched their preferences. My approach required integrating API data retrieval with user input to dynamically suggest the most suitable vacation spots based on real-time weather data.</p>
                    <p>Starting the project, I set up the necessary functions to fetch geographical coordinates and weather details from the API. I defined a where_to function to convert city names into latitude and longitude using OpenWeather's Geocoding API, and a get_weather function to retrieve current weather conditions based on these coordinates. This dual-function setup was crucial for accessing the most accurate and relevant data for each city entered by the user.</p>
                    <p>To ensure robustness and accuracy, I incorporated error handling and validations within the input phase, guiding users through the data entry process effectively. This ensured that the program could handle various input formats and discrepancies gracefully, maintaining a seamless user experience. Testing these functions rigorously helped me refine their functionality and ensure reliable performance under different scenarios.</p>
                    <p>One of the significant challenges was designing the logic to compare the current temperatures of the listed cities with the user's ideal temperature. This involved calculating the absolute differences and identifying the city with the minimum discrepancy. Implementing this feature was both challenging and rewarding, as it brought the core functionality of my vacation recommender to life, allowing it to provide personalized recommendations effectively.</p>
                    <p>As I wrapped up the project, I reflected on the integration of multiple programming concepts and tools—from API interaction and data handling to user interface design. This project not only enhanced my technical skills but also deepened my understanding of practical application development in Python. It was a fulfilling experience to see the program successfully recommend vacation destinations that aligned with user-defined weather preferences, showcasing the power of combining data retrieval and user input processing in real-world applications.</p>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>Copyright © 2024 by Jason Kemp | SI 339 Final Project | All rights reserved.</p>
    </footer>
</body>
</html>
